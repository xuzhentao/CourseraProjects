{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional callbacks\n",
    "\n",
    "In this reading we'll be looking at more of the inbuilt callbacks available in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again be using the sklearn diabetes dataset to demonstrate these callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the input and target variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and test sets\n",
    "\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also build a simple model to fit to the data with our callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(loss='mse',\n",
    "                optimizer=\"adam\",metrics=[\"mse\",\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the callbacks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduler\n",
    "\n",
    "**Usage:** `tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)`\n",
    "\n",
    "The learning rate scheduler that we implemented in the previous reading as a custom callback is also available as a built in callback. \n",
    "\n",
    "As in our custom callback, the `LearningRateScheduler` in Keras takes a function `schedule` as an argument. \n",
    "\n",
    "This function `schedule` should take two arguments:\n",
    "* The current epoch (as an integer), and\n",
    "* The current learning rate,\n",
    "\n",
    "and return new learning rate for that epoch. \n",
    "\n",
    "The `LearningRateScheduler` also has an optional `verbose` argument, which prints information about the learning rate if it is set to 1.\n",
    "\n",
    "Let's see a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedule function\n",
    "\n",
    "def lr_function(epoch, lr):\n",
    "    if epoch % 2 == 0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr + epoch/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0020000000474974513.\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.005000000094994903.\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.009999999888241292.\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.01699999977648258.\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.016999999061226845.\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025999999061226846.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_function, verbose=1)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use lambda functions to define your `schedule` given an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.3333333333333333.\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.125.\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.07692307692307693.\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05555555555555555.\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.043478260869565216.\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.03571428571428571.\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.030303030303030304.\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.02631578947368421.\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.023255813953488372.\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.020833333333333332.\n"
     ]
    }
   ],
   "source": [
    "# Train the model with a difference schedule\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lambda x:1/(3+5*x), verbose=1)], \n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV logger\n",
    "**Usage** `tf.keras.callbacks.CSVLogger(filename, separator=',', append=False)`\n",
    "\n",
    "This callback streams the results from each epoch into a CSV file.\n",
    "The first line of the CSV file will be the names of pieces of information recorded on each subsequent line, beginning with the epoch and loss value. The values of metrics at the end of each epoch will also be recorded.\n",
    "\n",
    "The only compulsory argument is the `filename` for the log to be streamed to. This could also be a filepath.\n",
    "\n",
    "You can also specify the `separator` to be used between entries on each line.\n",
    "\n",
    "The `append` argument allows you the option to append your results to an existing file with the same name. This can be particularly useful if you are continuing training.\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with a CSV logger\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10,\n",
    "                    callbacks=[tf.keras.callbacks.CSVLogger(\"results.csv\")], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information in the CSV file we have created using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2824.322837</td>\n",
       "      <td>42.807457</td>\n",
       "      <td>2824.3225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2835.831591</td>\n",
       "      <td>42.740406</td>\n",
       "      <td>2835.8313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2846.963566</td>\n",
       "      <td>43.024582</td>\n",
       "      <td>2846.9634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2823.049246</td>\n",
       "      <td>42.743183</td>\n",
       "      <td>2823.0490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2832.203126</td>\n",
       "      <td>42.812653</td>\n",
       "      <td>2832.2031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2840.809891</td>\n",
       "      <td>42.749683</td>\n",
       "      <td>2840.8100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2877.809301</td>\n",
       "      <td>43.031280</td>\n",
       "      <td>2877.8090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2883.720891</td>\n",
       "      <td>43.039200</td>\n",
       "      <td>2883.7207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2809.752531</td>\n",
       "      <td>42.470450</td>\n",
       "      <td>2809.7524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2792.212056</td>\n",
       "      <td>42.419262</td>\n",
       "      <td>2792.2120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              loss        mae        mse\n",
       "epoch                                   \n",
       "0      2824.322837  42.807457  2824.3225\n",
       "1      2835.831591  42.740406  2835.8313\n",
       "2      2846.963566  43.024582  2846.9634\n",
       "3      2823.049246  42.743183  2823.0490\n",
       "4      2832.203126  42.812653  2832.2031\n",
       "5      2840.809891  42.749683  2840.8100\n",
       "6      2877.809301  43.031280  2877.8090\n",
       "7      2883.720891  43.039200  2883.7207\n",
       "8      2809.752531  42.470450  2809.7524\n",
       "9      2792.212056  42.419262  2792.2120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"results.csv\", index_col='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda callbacks\n",
    "**Usage** `tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_begin=None, on_epoch_end=None, \n",
    "        on_batch_begin=None, on_batch_end=None, \n",
    "        on_train_begin=None, on_train_end=None)`\n",
    "\n",
    "Lambda callbacks are used to quickly define simple custom callbacks with the use of lambda functions.\n",
    "\n",
    "Each of the functions require some positional arguments.\n",
    "* `on_epoch_begin` and `on_epoch_end` expect two arguments: `epoch` and `logs`,\n",
    "* `on_batch_begin` and `on_batch_end` expect two arguments: `batch` and `logs` and\n",
    "* `on_train_begin` and `on_train_end` expect one argument: `logs`.\n",
    "\n",
    "Let's see an example of this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the epoch number at the beginning of each epoch\n",
    "\n",
    "epoch_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch,logs: print('Starting Epoch {}!'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loss at the end of each batch\n",
    "\n",
    "batch_loss_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_batch_end=lambda batch,logs: print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inform that training is finished\n",
    "\n",
    "train_finish_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_train_end=lambda logs: print('Training finished!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1!\n",
      "\n",
      " After batch 0, the loss is 2578.49.\n",
      "\n",
      " After batch 1, the loss is 3297.70.\n",
      "\n",
      " After batch 2, the loss is 2211.76.\n",
      "\n",
      " After batch 3, the loss is 3042.86.\n",
      "Starting Epoch 2!\n",
      "\n",
      " After batch 0, the loss is 2597.89.\n",
      "\n",
      " After batch 1, the loss is 2421.74.\n",
      "\n",
      " After batch 2, the loss is 3672.42.\n",
      "\n",
      " After batch 3, the loss is 2447.88.\n",
      "Starting Epoch 3!\n",
      "\n",
      " After batch 0, the loss is 2857.28.\n",
      "\n",
      " After batch 1, the loss is 3088.05.\n",
      "\n",
      " After batch 2, the loss is 2177.57.\n",
      "\n",
      " After batch 3, the loss is 2981.45.\n",
      "Starting Epoch 4!\n",
      "\n",
      " After batch 0, the loss is 2988.25.\n",
      "\n",
      " After batch 1, the loss is 3112.92.\n",
      "\n",
      " After batch 2, the loss is 2459.66.\n",
      "\n",
      " After batch 3, the loss is 2546.86.\n",
      "Starting Epoch 5!\n",
      "\n",
      " After batch 0, the loss is 2735.07.\n",
      "\n",
      " After batch 1, the loss is 2161.97.\n",
      "\n",
      " After batch 2, the loss is 2994.32.\n",
      "\n",
      " After batch 3, the loss is 3225.39.\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the lambda callbacks\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=5, batch_size=100,\n",
    "                    callbacks=[epoch_callback, batch_loss_callback,train_finish_callback], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce learning rate on plateau\n",
    "**Usage** `tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.1, \n",
    "            patience=10, \n",
    "            verbose=0, \n",
    "            mode='auto', \n",
    "            min_delta=0.0001, \n",
    "            cooldown=0, \n",
    "            min_lr=0)`\n",
    "\n",
    "The `ReduceLROnPlateau` callback allows reduction of the learning rate when a metric has stopped improving. \n",
    "The arguments are similar to those used in the `EarlyStopping` callback.\n",
    "* The argument `monitor` is used to specify which metric to base the callback on.\n",
    "* The `factor` is the factor by which the learning rate decreases i.e., new_lr=factor*old_lr.\n",
    "* The `patience` is the number of epochs where there is no improvement on the monitored metric before the learning rate is reduced.\n",
    "* The `verbose` argument will produce progress messages when set to 1.\n",
    "* The `mode` determines whether the learning rate will decrease when the monitored quantity stops increasing (`max`) or decreasing (`min`). The `auto` setting causes the callback to infer the mode from the monitored quantity.\n",
    "* The `min_delta` is the smallest change in the monitored quantity to be deemed an improvement.\n",
    "* The `cooldown` is the number of epochs to wait after the learning rate is changed before the callback resumes normal operation.\n",
    "* The `min_lr` is a lower bound on the learning rate that the callback will produce.\n",
    "\n",
    "Let's examine a final example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 397 samples\n",
      "Epoch 1/10000\n",
      "397/397 [==============================] - 0s 201us/sample - loss: 2768.2216 - mse: 2768.2217 - mae: 42.1676\n",
      "Epoch 2/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2770.0859 - mse: 2770.0859 - mae: 42.0776\n",
      "Epoch 3/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2771.9890 - mse: 2771.9890 - mae: 42.0818\n",
      "Epoch 4/10000\n",
      "397/397 [==============================] - 0s 238us/sample - loss: 2767.1893 - mse: 2767.1892 - mae: 42.0788\n",
      "Epoch 5/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2766.3799 - mse: 2766.3796 - mae: 42.1240\n",
      "Epoch 6/10000\n",
      "397/397 [==============================] - 0s 226us/sample - loss: 2766.5576 - mse: 2766.5576 - mae: 42.1354\n",
      "Epoch 7/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2766.7672 - mse: 2766.7673 - mae: 42.1428\n",
      "Epoch 8/10000\n",
      "397/397 [==============================] - 0s 237us/sample - loss: 2766.1190 - mse: 2766.1189 - mae: 42.0928\n",
      "Epoch 9/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2763.2829 - mse: 2763.2830 - mae: 42.0221\n",
      "Epoch 10/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2766.8972 - mse: 2766.8970 - mae: 42.0153\n",
      "Epoch 11/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2764.7363 - mse: 2764.7361 - mae: 42.0517\n",
      "Epoch 12/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2766.5278 - mse: 2766.5276 - mae: 42.0811\n",
      "Epoch 13/10000\n",
      "397/397 [==============================] - 0s 238us/sample - loss: 2761.5872 - mse: 2761.5872 - mae: 41.9943\n",
      "Epoch 14/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2760.3479 - mse: 2760.3479 - mae: 41.9852\n",
      "Epoch 15/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2760.2204 - mse: 2760.2205 - mae: 41.9946\n",
      "Epoch 16/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2762.5130 - mse: 2762.5132 - mae: 42.0542\n",
      "Epoch 17/10000\n",
      "397/397 [==============================] - 0s 238us/sample - loss: 2761.1088 - mse: 2761.1089 - mae: 42.0615\n",
      "Epoch 18/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2757.3980 - mse: 2757.3979 - mae: 41.9931\n",
      "Epoch 19/10000\n",
      "397/397 [==============================] - 0s 232us/sample - loss: 2776.3421 - mse: 2776.3423 - mae: 42.0580\n",
      "Epoch 20/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2758.8599 - mse: 2758.8601 - mae: 41.9108\n",
      "Epoch 21/10000\n",
      "397/397 [==============================] - 0s 228us/sample - loss: 2755.6650 - mse: 2755.6650 - mae: 42.0018\n",
      "Epoch 22/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2762.8978 - mse: 2762.8979 - mae: 42.0250\n",
      "Epoch 23/10000\n",
      "397/397 [==============================] - 0s 240us/sample - loss: 2757.2949 - mse: 2757.2949 - mae: 41.9859\n",
      "Epoch 24/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2758.2936 - mse: 2758.2935 - mae: 41.9593\n",
      "Epoch 25/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2768.7061 - mse: 2768.7058 - mae: 42.0882\n",
      "Epoch 26/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2754.9863 - mse: 2754.9866 - mae: 41.9613\n",
      "Epoch 27/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2752.9397 - mse: 2752.9399 - mae: 41.9120\n",
      "Epoch 28/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2756.1779 - mse: 2756.1780 - mae: 41.9219\n",
      "Epoch 29/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2752.0203 - mse: 2752.0205 - mae: 41.9197\n",
      "Epoch 30/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2751.9859 - mse: 2751.9858 - mae: 41.9037\n",
      "Epoch 31/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2750.5356 - mse: 2750.5356 - mae: 41.8865\n",
      "Epoch 32/10000\n",
      "397/397 [==============================] - 0s 237us/sample - loss: 2757.9731 - mse: 2757.9729 - mae: 41.9002\n",
      "Epoch 33/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2749.9049 - mse: 2749.9050 - mae: 41.8586\n",
      "Epoch 34/10000\n",
      "397/397 [==============================] - 0s 232us/sample - loss: 2761.7252 - mse: 2761.7251 - mae: 42.0947\n",
      "Epoch 35/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2749.8856 - mse: 2749.8855 - mae: 41.9648\n",
      "Epoch 36/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2750.7801 - mse: 2750.7803 - mae: 41.9021\n",
      "Epoch 37/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2759.7000 - mse: 2759.7000 - mae: 41.8745\n",
      "Epoch 38/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2745.1169 - mse: 2745.1167 - mae: 41.8150\n",
      "Epoch 39/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2762.0688 - mse: 2762.0686 - mae: 42.1100\n",
      "Epoch 40/10000\n",
      "397/397 [==============================] - 0s 230us/sample - loss: 2756.1587 - mse: 2756.1587 - mae: 42.0069\n",
      "Epoch 41/10000\n",
      "397/397 [==============================] - 0s 239us/sample - loss: 2762.0463 - mse: 2762.0464 - mae: 41.9260\n",
      "Epoch 42/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2754.3127 - mse: 2754.3127 - mae: 41.8980\n",
      "Epoch 43/10000\n",
      "397/397 [==============================] - 0s 231us/sample - loss: 2743.9893 - mse: 2743.9893 - mae: 41.8382\n",
      "Epoch 44/10000\n",
      "397/397 [==============================] - 0s 19us/sample - loss: 2749.3127 - mse: 2749.3127 - mae: 41.9142\n",
      "Epoch 45/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2746.7628 - mse: 2746.7629 - mae: 41.8919\n",
      "Epoch 46/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2749.9484 - mse: 2749.9482 - mae: 41.9216\n",
      "Epoch 47/10000\n",
      "397/397 [==============================] - 0s 241us/sample - loss: 2747.2041 - mse: 2747.2041 - mae: 41.8321\n",
      "Epoch 48/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2748.9199 - mse: 2748.9199 - mae: 41.8349\n",
      "Epoch 49/10000\n",
      "397/397 [==============================] - 0s 232us/sample - loss: 2744.7414 - mse: 2744.7415 - mae: 41.8552\n",
      "Epoch 50/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2747.0262 - mse: 2747.0261 - mae: 41.9062\n",
      "Epoch 51/10000\n",
      "397/397 [==============================] - 0s 234us/sample - loss: 2743.6795 - mse: 2743.6794 - mae: 41.8741\n",
      "Epoch 52/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2746.2239 - mse: 2746.2239 - mae: 41.8153\n",
      "Epoch 53/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2752.0451 - mse: 2752.0449 - mae: 41.7871\n",
      "Epoch 54/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2739.7552 - mse: 2739.7551 - mae: 41.7598\n",
      "Epoch 55/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2744.1647 - mse: 2744.1650 - mae: 41.8714\n",
      "Epoch 56/10000\n",
      "397/397 [==============================] - 0s 240us/sample - loss: 2746.5737 - mse: 2746.5737 - mae: 41.9275\n",
      "Epoch 57/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2742.8441 - mse: 2742.8442 - mae: 41.8435\n",
      "Epoch 58/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2741.7118 - mse: 2741.7117 - mae: 41.7772\n",
      "Epoch 59/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2745.1095 - mse: 2745.1096 - mae: 41.8455\n",
      "Epoch 60/10000\n",
      "397/397 [==============================] - 0s 229us/sample - loss: 2743.7664 - mse: 2743.7664 - mae: 41.8319\n",
      "Epoch 61/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2746.3068 - mse: 2746.3066 - mae: 41.8859\n",
      "Epoch 62/10000\n",
      "397/397 [==============================] - 0s 238us/sample - loss: 2739.6480 - mse: 2739.6477 - mae: 41.8256\n",
      "Epoch 63/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2743.7707 - mse: 2743.7708 - mae: 41.7780\n",
      "Epoch 64/10000\n",
      "397/397 [==============================] - 0s 232us/sample - loss: 2745.6993 - mse: 2745.6992 - mae: 41.7855\n",
      "Epoch 65/10000\n",
      "397/397 [==============================] - 0s 239us/sample - loss: 2740.7523 - mse: 2740.7524 - mae: 41.8366\n",
      "Epoch 66/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2737.3353 - mse: 2737.3354 - mae: 41.7963\n",
      "Epoch 67/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2744.2651 - mse: 2744.2651 - mae: 41.8180\n",
      "Epoch 68/10000\n",
      "397/397 [==============================] - 0s 229us/sample - loss: 2737.8572 - mse: 2737.8569 - mae: 41.7559\n",
      "Epoch 69/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2736.2417 - mse: 2736.2417 - mae: 41.7568\n",
      "Epoch 70/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2738.9921 - mse: 2738.9922 - mae: 41.8762\n",
      "Epoch 71/10000\n",
      "397/397 [==============================] - 0s 232us/sample - loss: 2739.4399 - mse: 2739.4402 - mae: 41.8469\n",
      "Epoch 72/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2741.9943 - mse: 2741.9944 - mae: 41.8316\n",
      "Epoch 73/10000\n",
      "397/397 [==============================] - 0s 240us/sample - loss: 2732.2627 - mse: 2732.2627 - mae: 41.7460\n",
      "Epoch 74/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2745.1109 - mse: 2745.1108 - mae: 41.7687\n",
      "Epoch 75/10000\n",
      "397/397 [==============================] - 0s 230us/sample - loss: 2734.0430 - mse: 2734.0427 - mae: 41.6757\n",
      "Epoch 76/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2734.4139 - mse: 2734.4138 - mae: 41.7411\n",
      "Epoch 77/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2735.5870 - mse: 2735.5869 - mae: 41.7828\n",
      "Epoch 78/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2734.5662 - mse: 2734.5662 - mae: 41.8137\n",
      "Epoch 79/10000\n",
      "397/397 [==============================] - 0s 231us/sample - loss: 2737.2580 - mse: 2737.2583 - mae: 41.7812\n",
      "Epoch 80/10000\n",
      "397/397 [==============================] - 0s 239us/sample - loss: 2731.9353 - mse: 2731.9351 - mae: 41.7103\n",
      "Epoch 81/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2733.0864 - mse: 2733.0862 - mae: 41.7316\n",
      "Epoch 82/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2746.0770 - mse: 2746.0769 - mae: 41.8848\n",
      "Epoch 83/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2737.1699 - mse: 2737.1699 - mae: 41.7609\n",
      "Epoch 84/10000\n",
      "397/397 [==============================] - 0s 234us/sample - loss: 2730.7639 - mse: 2730.7642 - mae: 41.6613\n",
      "Epoch 85/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2732.1155 - mse: 2732.1155 - mae: 41.7083\n",
      "Epoch 86/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2733.9111 - mse: 2733.9111 - mae: 41.7442\n",
      "Epoch 87/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2736.8446 - mse: 2736.8445 - mae: 41.7135\n",
      "Epoch 88/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2728.8390 - mse: 2728.8391 - mae: 41.6425\n",
      "Epoch 89/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2735.0593 - mse: 2735.0591 - mae: 41.8006\n",
      "Epoch 90/10000\n",
      "397/397 [==============================] - 0s 234us/sample - loss: 2728.8590 - mse: 2728.8589 - mae: 41.7226\n",
      "Epoch 91/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2730.5817 - mse: 2730.5818 - mae: 41.6578\n",
      "Epoch 92/10000\n",
      "397/397 [==============================] - 0s 231us/sample - loss: 2726.6516 - mse: 2726.6519 - mae: 41.6343\n",
      "Epoch 93/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2732.7660 - mse: 2732.7661 - mae: 41.7180\n",
      "Epoch 94/10000\n",
      "397/397 [==============================] - 0s 232us/sample - loss: 2729.7182 - mse: 2729.7183 - mae: 41.6526\n",
      "Epoch 95/10000\n",
      "397/397 [==============================] - 0s 239us/sample - loss: 2725.5937 - mse: 2725.5938 - mae: 41.6431\n",
      "Epoch 96/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2728.1493 - mse: 2728.1492 - mae: 41.6641\n",
      "Epoch 97/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2728.1562 - mse: 2728.1562 - mae: 41.6527\n",
      "Epoch 98/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2732.4340 - mse: 2732.4341 - mae: 41.6895\n",
      "Epoch 99/10000\n",
      "397/397 [==============================] - 0s 234us/sample - loss: 2746.3474 - mse: 2746.3477 - mae: 41.8143\n",
      "Epoch 100/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2726.6198 - mse: 2726.6196 - mae: 41.6037\n",
      "Epoch 101/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2725.6475 - mse: 2725.6475 - mae: 41.6520\n",
      "Epoch 102/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2725.8614 - mse: 2725.8616 - mae: 41.7322\n",
      "Epoch 103/10000\n",
      "397/397 [==============================] - 0s 230us/sample - loss: 2721.2565 - mse: 2721.2566 - mae: 41.6316\n",
      "Epoch 104/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2725.1597 - mse: 2725.1597 - mae: 41.6151\n",
      "Epoch 105/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2724.5511 - mse: 2724.5513 - mae: 41.6393\n",
      "Epoch 106/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2724.1222 - mse: 2724.1221 - mae: 41.6296\n",
      "Epoch 107/10000\n",
      "397/397 [==============================] - 0s 231us/sample - loss: 2722.8387 - mse: 2722.8389 - mae: 41.6242\n",
      "Epoch 108/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2723.9025 - mse: 2723.9023 - mae: 41.6192\n",
      "Epoch 109/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2720.2213 - mse: 2720.2214 - mae: 41.6053\n",
      "Epoch 110/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2726.1109 - mse: 2726.1108 - mae: 41.6770\n",
      "Epoch 111/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2719.2626 - mse: 2719.2627 - mae: 41.6099\n",
      "Epoch 112/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2717.2756 - mse: 2717.2754 - mae: 41.5569\n",
      "Epoch 113/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2726.1240 - mse: 2726.1240 - mae: 41.5797\n",
      "Epoch 114/10000\n",
      "397/397 [==============================] - 0s 234us/sample - loss: 2722.7702 - mse: 2722.7703 - mae: 41.5741\n",
      "Epoch 115/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2721.2195 - mse: 2721.2195 - mae: 41.6391\n",
      "Epoch 116/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2726.2044 - mse: 2726.2043 - mae: 41.6423\n",
      "Epoch 117/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2717.4956 - mse: 2717.4956 - mae: 41.5762\n",
      "Epoch 118/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2721.1862 - mse: 2721.1860 - mae: 41.6423\n",
      "Epoch 119/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2717.6103 - mse: 2717.6106 - mae: 41.5937\n",
      "Epoch 120/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2726.2030 - mse: 2726.2029 - mae: 41.5986\n",
      "Epoch 121/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2730.2684 - mse: 2730.2683 - mae: 41.6608\n",
      "Epoch 122/10000\n",
      "100/397 [======>.......................] - ETA: 0s - loss: 2834.6064 - mse: 2834.6064 - mae: 42.6316\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 0.00416666679084301.\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2723.3814 - mse: 2723.3813 - mae: 41.6015\n",
      "Epoch 123/10000\n",
      "397/397 [==============================] - 0s 238us/sample - loss: 2714.7105 - mse: 2714.7104 - mae: 41.5535\n",
      "Epoch 124/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2715.6149 - mse: 2715.6150 - mae: 41.5581\n",
      "Epoch 125/10000\n",
      "397/397 [==============================] - 0s 238us/sample - loss: 2714.9447 - mse: 2714.9448 - mae: 41.5547\n",
      "Epoch 126/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2715.7546 - mse: 2715.7546 - mae: 41.5762\n",
      "Epoch 127/10000\n",
      "397/397 [==============================] - 0s 230us/sample - loss: 2714.5512 - mse: 2714.5513 - mae: 41.5598\n",
      "Epoch 128/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2714.8874 - mse: 2714.8872 - mae: 41.5586\n",
      "Epoch 129/10000\n",
      "397/397 [==============================] - 0s 232us/sample - loss: 2716.7013 - mse: 2716.7012 - mae: 41.5650\n",
      "Epoch 130/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2714.4412 - mse: 2714.4412 - mae: 41.5442\n",
      "Epoch 131/10000\n",
      "397/397 [==============================] - 0s 227us/sample - loss: 2715.3901 - mse: 2715.3901 - mae: 41.5501\n",
      "Epoch 132/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2714.3516 - mse: 2714.3513 - mae: 41.5466\n",
      "Epoch 133/10000\n",
      "397/397 [==============================] - 0s 234us/sample - loss: 2714.8899 - mse: 2714.8899 - mae: 41.5514\n",
      "Epoch 134/10000\n",
      "397/397 [==============================] - 0s 242us/sample - loss: 2715.3183 - mse: 2715.3184 - mae: 41.5562\n",
      "Epoch 135/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2715.5930 - mse: 2715.5933 - mae: 41.5473\n",
      "Epoch 136/10000\n",
      "397/397 [==============================] - 0s 239us/sample - loss: 2714.4916 - mse: 2714.4917 - mae: 41.5425\n",
      "Epoch 137/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2715.7861 - mse: 2715.7861 - mae: 41.5616\n",
      "Epoch 138/10000\n",
      "397/397 [==============================] - 0s 231us/sample - loss: 2714.2248 - mse: 2714.2249 - mae: 41.5451\n",
      "Epoch 139/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2716.4870 - mse: 2716.4868 - mae: 41.5479\n",
      "Epoch 140/10000\n",
      "397/397 [==============================] - 0s 231us/sample - loss: 2714.5298 - mse: 2714.5300 - mae: 41.5415\n",
      "Epoch 141/10000\n",
      "397/397 [==============================] - 0s 238us/sample - loss: 2713.9367 - mse: 2713.9368 - mae: 41.5323\n",
      "Epoch 142/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2715.6097 - mse: 2715.6096 - mae: 41.5387\n",
      "Epoch 143/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2714.0804 - mse: 2714.0806 - mae: 41.5345\n",
      "Epoch 144/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2718.1715 - mse: 2718.1716 - mae: 41.5695\n",
      "Epoch 145/10000\n",
      "397/397 [==============================] - 0s 232us/sample - loss: 2715.7223 - mse: 2715.7224 - mae: 41.5510\n",
      "Epoch 146/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2715.9492 - mse: 2715.9492 - mae: 41.5543\n",
      "Epoch 147/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2714.1298 - mse: 2714.1296 - mae: 41.5306\n",
      "Epoch 148/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2713.9984 - mse: 2713.9985 - mae: 41.5357\n",
      "Epoch 149/10000\n",
      "397/397 [==============================] - 0s 238us/sample - loss: 2715.4351 - mse: 2715.4351 - mae: 41.5611\n",
      "Epoch 150/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2713.6969 - mse: 2713.6970 - mae: 41.5465\n",
      "Epoch 151/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2714.4982 - mse: 2714.4980 - mae: 41.5430\n",
      "Epoch 152/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2713.7562 - mse: 2713.7563 - mae: 41.5320\n",
      "Epoch 153/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2714.1118 - mse: 2714.1118 - mae: 41.5309\n",
      "Epoch 154/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2713.7402 - mse: 2713.7400 - mae: 41.5350\n",
      "Epoch 155/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2713.1798 - mse: 2713.1797 - mae: 41.5328\n",
      "Epoch 156/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2713.2177 - mse: 2713.2175 - mae: 41.5334\n",
      "Epoch 157/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2713.4263 - mse: 2713.4263 - mae: 41.5289\n",
      "Epoch 158/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2713.5201 - mse: 2713.5203 - mae: 41.5301\n",
      "Epoch 159/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2714.3819 - mse: 2714.3818 - mae: 41.5357\n",
      "Epoch 160/10000\n",
      "397/397 [==============================] - 0s 231us/sample - loss: 2713.5999 - mse: 2713.5999 - mae: 41.5280\n",
      "Epoch 161/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2713.1866 - mse: 2713.1868 - mae: 41.5194\n",
      "Epoch 162/10000\n",
      "397/397 [==============================] - 0s 234us/sample - loss: 2714.1682 - mse: 2714.1685 - mae: 41.5296\n",
      "Epoch 163/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2713.4316 - mse: 2713.4316 - mae: 41.5270\n",
      "Epoch 164/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2713.3483 - mse: 2713.3481 - mae: 41.5358\n",
      "Epoch 165/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.8579 - mse: 2712.8577 - mae: 41.5290\n",
      "Epoch 166/10000\n",
      "397/397 [==============================] - 0s 229us/sample - loss: 2713.1947 - mse: 2713.1946 - mae: 41.5328\n",
      "Epoch 167/10000\n",
      "397/397 [==============================] - 0s 240us/sample - loss: 2715.7856 - mse: 2715.7856 - mae: 41.5533\n",
      "Epoch 168/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2713.0276 - mse: 2713.0273 - mae: 41.5393\n",
      "Epoch 169/10000\n",
      "397/397 [==============================] - 0s 237us/sample - loss: 2714.6153 - mse: 2714.6152 - mae: 41.5431\n",
      "Epoch 170/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2714.8215 - mse: 2714.8218 - mae: 41.5354\n",
      "Epoch 171/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2714.8021 - mse: 2714.8022 - mae: 41.5313\n",
      "Epoch 172/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2712.6096 - mse: 2712.6096 - mae: 41.5261\n",
      "Epoch 173/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2712.5474 - mse: 2712.5471 - mae: 41.5261\n",
      "Epoch 174/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2713.4174 - mse: 2713.4175 - mae: 41.5334\n",
      "Epoch 175/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2714.5143 - mse: 2714.5144 - mae: 41.5521\n",
      "Epoch 176/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2712.9390 - mse: 2712.9390 - mae: 41.5293\n",
      "Epoch 177/10000\n",
      "397/397 [==============================] - 0s 231us/sample - loss: 2712.4342 - mse: 2712.4341 - mae: 41.5270\n",
      "Epoch 178/10000\n",
      "397/397 [==============================] - 0s 237us/sample - loss: 2713.4452 - mse: 2713.4453 - mae: 41.5281\n",
      "Epoch 179/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2713.4888 - mse: 2713.4888 - mae: 41.5252\n",
      "Epoch 180/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2712.7609 - mse: 2712.7610 - mae: 41.5209\n",
      "Epoch 181/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2714.9751 - mse: 2714.9751 - mae: 41.5402\n",
      "Epoch 182/10000\n",
      "397/397 [==============================] - 0s 236us/sample - loss: 2712.4059 - mse: 2712.4058 - mae: 41.5216\n",
      "Epoch 183/10000\n",
      "397/397 [==============================] - 0s 240us/sample - loss: 2712.5948 - mse: 2712.5947 - mae: 41.5224\n",
      "Epoch 184/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2713.5709 - mse: 2713.5708 - mae: 41.5251\n",
      "Epoch 185/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.7497 - mse: 2712.7498 - mae: 41.5186\n",
      "Epoch 186/10000\n",
      "397/397 [==============================] - 0s 229us/sample - loss: 2712.6648 - mse: 2712.6650 - mae: 41.5296\n",
      "Epoch 187/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.2661 - mse: 2712.2661 - mae: 41.5242\n",
      "Epoch 188/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2713.1874 - mse: 2713.1877 - mae: 41.5313\n",
      "Epoch 189/10000\n",
      "397/397 [==============================] - 0s 240us/sample - loss: 2712.4452 - mse: 2712.4453 - mae: 41.5243\n",
      "Epoch 190/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.1089 - mse: 2712.1089 - mae: 41.5178\n",
      "Epoch 191/10000\n",
      "397/397 [==============================] - 0s 241us/sample - loss: 2712.2165 - mse: 2712.2166 - mae: 41.5167\n",
      "Epoch 192/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2712.9021 - mse: 2712.9021 - mae: 41.5179\n",
      "Epoch 193/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.9976 - mse: 2712.9976 - mae: 41.5224\n",
      "Epoch 194/10000\n",
      "397/397 [==============================] - 0s 240us/sample - loss: 2715.2300 - mse: 2715.2297 - mae: 41.5338\n",
      "Epoch 195/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.5233 - mse: 2712.5232 - mae: 41.5236\n",
      "Epoch 196/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2712.0823 - mse: 2712.0823 - mae: 41.5123\n",
      "Epoch 197/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2711.8673 - mse: 2711.8672 - mae: 41.5068\n",
      "Epoch 198/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2712.5299 - mse: 2712.5300 - mae: 41.5100\n",
      "Epoch 199/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.9516 - mse: 2712.9514 - mae: 41.5235\n",
      "Epoch 200/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2711.8496 - mse: 2711.8496 - mae: 41.5093\n",
      "Epoch 201/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2712.4574 - mse: 2712.4573 - mae: 41.5130\n",
      "Epoch 202/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2711.4285 - mse: 2711.4285 - mae: 41.5063\n",
      "Epoch 203/10000\n",
      "397/397 [==============================] - 0s 233us/sample - loss: 2711.8294 - mse: 2711.8293 - mae: 41.5132\n",
      "Epoch 204/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.9581 - mse: 2712.9580 - mae: 41.5226\n",
      "Epoch 205/10000\n",
      "397/397 [==============================] - 0s 230us/sample - loss: 2712.5656 - mse: 2712.5654 - mae: 41.5280\n",
      "Epoch 206/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.8067 - mse: 2712.8066 - mae: 41.5191\n",
      "Epoch 207/10000\n",
      "397/397 [==============================] - 0s 237us/sample - loss: 2711.8245 - mse: 2711.8242 - mae: 41.5096\n",
      "Epoch 208/10000\n",
      "397/397 [==============================] - 0s 17us/sample - loss: 2712.1282 - mse: 2712.1282 - mae: 41.5115\n",
      "Epoch 209/10000\n",
      "397/397 [==============================] - 0s 235us/sample - loss: 2712.0820 - mse: 2712.0818 - mae: 41.5101\n",
      "Epoch 210/10000\n",
      "397/397 [==============================] - 0s 16us/sample - loss: 2711.7966 - mse: 2711.7966 - mae: 41.5120\n",
      "Epoch 211/10000\n",
      "397/397 [==============================] - 0s 237us/sample - loss: 2712.0853 - mse: 2712.0854 - mae: 41.5162\n",
      "Epoch 212/10000\n",
      "100/397 [======>.......................] - ETA: 0s - loss: 2386.2046 - mse: 2386.2046 - mae: 39.6111\n",
      "Epoch 00212: ReduceLROnPlateau reducing learning rate to 0.0008333333767950535.\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2712.4106 - mse: 2712.4106 - mae: 41.5229\n",
      "Epoch 213/10000\n",
      "397/397 [==============================] - 0s 230us/sample - loss: 2711.0582 - mse: 2711.0583 - mae: 41.5082\n",
      "Epoch 214/10000\n",
      "397/397 [==============================] - 0s 239us/sample - loss: 2710.9797 - mse: 2710.9795 - mae: 41.5073\n",
      "Epoch 215/10000\n",
      "397/397 [==============================] - 0s 18us/sample - loss: 2711.1120 - mse: 2711.1121 - mae: 41.5077\n",
      "Epoch 216/10000\n",
      "100/397 [======>.......................] - ETA: 0s - loss: 2404.1057 - mse: 2404.1057 - mae: 39.6272"
     ]
    }
   ],
   "source": [
    "# Train the model with the ReduceLROnPlateau callback\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=10000, batch_size=100,\n",
    "                    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                        monitor=\"loss\",factor=0.2, verbose=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading and resources\n",
    "* https://keras.io/callbacks/\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CSVLogger\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
